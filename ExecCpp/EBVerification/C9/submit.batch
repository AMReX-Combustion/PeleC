#!/bin/bash

#SBATCH --job-name=pelec_eb_channel
#SBATCH --account=exact
#SBATCH --nodes=1
#SBATCH --time=1:00:00
#SBATCH --partition=debug
#SBATCH -o %x.o%j

# module purge
# MODULES=modules
# module unuse ${MODULEPATH}
# module use /nopt/nrel/ecom/hpacf/compilers/${MODULES}
# module use /nopt/nrel/ecom/hpacf/utilities/${MODULES}
# module use /nopt/nrel/ecom/hpacf/software/${MODULES}/gcc-7.4.0

# module load gcc
# module load git
# module load mpich

# ranks_per_node=36
# mpi_ranks=$(expr $SLURM_JOB_NUM_NODES \* $ranks_per_node)
# export OMP_NUM_THREADS=1  # Max hardware threads = 4
# export OMP_PLACES=threads
# export OMP_PROC_BIND=spread

# echo "Job name       = $SLURM_JOB_NAME"
# echo "Num. nodes     = $SLURM_JOB_NUM_NODES"
# echo "Num. MPI Ranks = $mpi_ranks"
# echo "Num. threads   = $OMP_NUM_THREADS"
# echo "Working dir    = $PWD"


paren=`pwd`
pelec="${paren}/PeleC3d.llvm.MPI.ex"

res=( 4 8 )
for i in "${res[@]}"
do
    rm -rf "${i}"
    mkdir "${i}"
    cd "${i}" || exit
    cp "${paren}/inputs_3d" .
    ny="$((i*4))"
    nx="$((i*4*5))"
    mpiexec -n 1 "${pelec}" inputs_3d amr.n_cell="${nx} ${ny} ${ny}" > out
    #srun -n 4 "${pelec}" inputs_3d amr.n_cell="${nx} ${ny} ${ny}" > out
    ls -1v *plt*/Header | tee movie.visit
    cd "${paren}" || exit
done

for i in "${res[@]}"
do
    python3 pp.py -f "${i}"
done
